import json
import os
import sys
import argparse
from typing import Optional, Any, Type
from pydantic import BaseModel, create_model

from .core import generate


def _load_schema(schema_path: str) -> type[BaseModel]:
    with open(schema_path, "r", encoding="utf-8") as f:
        schema = json.load(f)
    # Expecting a JSON Schema generated by Pydantic; create a simple dynamic model as a fallback
    # For a minimal CLI, assume top-level object with properties and basic types
    title = schema.get("title", "StructsureModel")
    properties = schema.get("properties", {})
    required = set(schema.get("required", []))

    fields = {}
    for name, spec in properties.items():
        typ = spec.get("type", "string")
        py_type: object = str
        if typ == "integer":
            py_type = int
        elif typ == "number":
            py_type = float
        elif typ == "boolean":
            py_type = bool
        elif typ == "array":
            py_type = list
        elif typ == "object":
            py_type = dict
        default = ... if name in required else None
        fields[name] = (py_type, default)

    return create_model(title, **fields)  # type: ignore[arg-type]


def main(argv: Optional[list[str]] = None) -> int:
    parser = argparse.ArgumentParser(prog="structsure", description="Generate validated structured JSON from an LLM")
    parser.add_argument("prompt", help="User prompt text or @file to read from a file")
    parser.add_argument("--schema", required=False, help="Path to a JSON Schema file (from Pydantic)")
    parser.add_argument("--model", default=None, help="Model name (default: llama3 locally or gpt-4o if OPENAI_API_KEY is set)")
    parser.add_argument("--provider", choices=["ollama", "openai"], default=None, help="Backend provider (auto-detect by default)")
    parser.add_argument("--retries", type=int, default=3, help="Max retries for self-correction")

    args = parser.parse_args(argv)

    provider = args.provider or ("openai" if os.environ.get("OPENAI_API_KEY") else "ollama")
    model = args.model or ("gpt-4o" if provider == "openai" else "llama3")

    if args.prompt.startswith("@"):
        with open(args.prompt[1:], "r", encoding="utf-8") as f:
            prompt = f.read()
    else:
        prompt = args.prompt

    if args.schema:
        ResponseModel: Type[BaseModel] = _load_schema(args.schema)  # type: ignore[assignment]
    else:
        class ResponseModel(BaseModel):
            content: str

    obj = generate(
        client=None,
        model=model,
        response_model=ResponseModel,
        prompt=prompt,
        max_retries=args.retries,
        provider=provider,
    )

    print(obj.model_dump_json(indent=2))
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
